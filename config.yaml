# Options for snowballing technique
snowball:
  # Upper limit for the number of papers to initially start with to download from OpenAlex and get citations for
  # Remove key for no upper limit
  # Default: 10
  seed_paper_limit: ''

  # Number of rounds of snowballing to do
  # Default: 5
  rounds: ''

  # For each round, the number of papers to snowball (fetch citation details for)
  # Remove key for no upper limit
  # Default: 10
  round_quota: ''

  # Minimum similarity score between the natural language query and title / abstract.
  # Must be [-1,1], but 0.4 is recommended.
  # Increasing will reduce the number of valid papers, potentially limiting the snowball pool.
  # Decreasing will increase the number of valid papers, potentially adding a lot of noise to the pool
  # It is recommended to keep this value somewhat low to cast a wide net, after which the score can be
  # increased when ranking papers
  # Default: 0.4
  min_similarity_score: ''

# Options for ranking papers
abstract_ranking:

  # Average tokens per word
  # Default: 1.2
  tokens_per_word: ''

  # Minimum similarity score between the prompt and abstract, must be [-1,1], but 0.4 recommended
  # Lowering this value will increase the number of papers included in the LLM ranking
  # It is recommended to keep this value relatively high so rounds aren't wasted on irrelevant papers
  # Default: 0.6
  min_abstract_score: ''

  # Number of papers to rank
  # All abstracts will need to fit inside context window for final ranking.
  # For reference, 1 abstract ~= 250 - 500 tokens
  # Default: 10
  top_n_papers: ''

# Config for OpenAlex API
# https://docs.openalex.org/
openalex:
  # Optional email to use for 'polite pool' for better access
  # Queries are 10x faster if an email is provided
  # https://docs.openalex.org/how-to-use-the-api/rate-limits-and-authentication#the-polite-pool
  # (env: SS_OA_EMAIL)
  email: ''

# Config for Grobid
# https://github.com/kermitt2/grobid
grobid:
  # Config for grobid python client
  # https://github.com/kermitt2/grobid-client-python?tab=readme-ov-file#default-configuration
  client_config:
    # Host and port of a grobid server
    # Useful if grobid server is not running locally, ie running on a remote host with more resources available
    # Default: localhost:8070
    # (env: SS_GROBID_SERVER)
    grobid_server: ''

    # Thread pool size.
    # Tune carefully: a large batch size will result in the data being written less frequently
    # Default: 1000
    batch_size: ''

    # Wait time when server is busy (seconds)
    # Default: 5
    sleep_time: ''

    # Client-side timeout (seconds)
    # Default: 180
    timeout: ''

    # List of TEI XML elements for which GROBID should extract coordinates (bounding boxes), ie x y location in pdf.
    # Defaults are
    # - persName
    # - figure
    # - ref
    # - biblStruct
    # - formula
    # - s
    coordinates: ''

  # Max number of requests allowed to be made to the grobid server at a time
  # Default: 1
  max_grobid_requests: ''

  # Max number of PDFs to allowed to download at the same time
  # Default: 10
  max_concurrent_downloads: ''

  # Max number of PDFs to be downloaded locally at one time
  # Default: 100
  max_local_pdfs: ''

# Config for Ollama Server
# Ignored if the OPENAI_API_KEY env variable is set, in which case the OpenAI API is used
ollama:
  # Useful if ollama server is not running locally, ie running on a remote host with more resources available
  # Default: localhost
  # (env: SS_OLLAMA_HOST)
  ollama_host: ''

  # Useful if ollama server is not running locally, ie running on a remote host with more resources available
  # Default: 11434
  # (env: SS_OLLAMA_PORT)
  ollama_port: ''

# Config for lLM
agent:
    # Model to use for generating queries and ranking abstracts
    # If using ollama (default), search for desired models here: https://ollama.com/search
    # If using OpenAI, search for desired models here: https://platform.openai.com/docs/models
    # Default: llama3
    # (env: SS_AGENT_MODEL)
    model_name: ''

    # If using ollama, specify a tag model to use
    # Default: latest
    # (env: SS_AGENT_TAG)
    model_tag: ''

    # Context window of selected model
    # Default: 5000 - conservative estimate
    # (env: SS_AGENT_CONTEXT_WINDOW)
    context_window: ''